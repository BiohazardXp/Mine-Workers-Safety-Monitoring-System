# Miner Safety ML Service

This Python module adds machine learning and analytical capabilities to the Miner Safety Management System.
It ingests the exported CSV files (generated by the React frontend) and produces:

- Cleaned & aggregated feature tables
- Regression and baseline predictive models
- Anomaly detection using Isolation Forest
- Feature importance summaries
- Trend extraction endpoints
- On-demand retraining via API

## Structure
```
python/
  config.py            # Configuration & paths
  requirements.txt     # Python dependencies
  main.py              # Service entrypoint (FastAPI + Uvicorn)
  ml/
    features.py        # Loading, cleaning, feature engineering
    pipelines.py       # Model pipeline factories & training helpers
    evaluate.py        # Metrics & model insight utilities
  models/
    train.py           # Orchestrates training & model registry updates
  services/
    api.py             # FastAPI endpoints
  utils/
    logging_config.py  # Rotating file logging setup
    io.py              # CSV discovery & loading helpers
  data/                # (Created automatically) place exported CSV files here
  logs/                # Service logs
  models/              # Serialized model artifacts & registry.json
```

## Quick Start
1. Ensure you have Python 3.11+ (recommended) installed.
2. From project root:
```bash
cd python
pip install -r requirements.txt
python -m main  # or: python main.py
```
Service will start on http://0.0.0.0:8100 (configurable via env vars \`ML_API_HOST\` and \`ML_API_PORT\`).

## API Endpoints
| Method | Path               | Description |
|--------|--------------------|-------------|
| GET    | /health            | Service liveness check |
| POST   | /train             | Build/refresh models from current CSVs |
| GET    | /summary           | Data/feature table summary |
| GET    | /feature-importance?model=regression | Feature importance for a model |
| GET    | /anomalies?limit=50| Top anomalous time windows |
| GET    | /trend?parameter=param_x | Recent trend values for a feature |

## Data Expectations
Each CSV row should follow the export schema implemented on the frontend:
```
 timestamp,date,time,device_id,device_name,employee_id,employee_name,category,parameter,value
```
Place these CSV files inside `python/data/` before calling `/train`.

## Model Artifacts
After training, serialized pipelines are saved to `python/models/`:
- baseline.joblib (linear regression baseline)
- regression.joblib (random forest)
- anomaly.joblib (isolation forest)

A running history is appended to `models/registry.json`.

## Extending
- Add new feature engineering steps in `ml/features.py`.
- Additional model types can be added in `ml/pipelines.py`.
- Add new analytical endpoints in `services/api.py`.

## Environment Variables
| Variable | Default | Purpose |
|----------|---------|---------|
| ML_API_HOST | 0.0.0.0 | Bind host |
| ML_API_PORT | 8100    | Port |
| AGG_INTERVAL | 5min  | Resample window for aggregation |
| N_JOBS       | 1     | Parallelism hint |

## Next Ideas
- Persist predictions & anomaly scores to a database
- Integrate with existing Node.js server via internal HTTP
- Add model performance dashboards
- Implement authentication for training endpoints
- Add forecasting (Prophet / ARIMA) for forward risk prediction

---
MIT License (add a LICENSE file if distributing)
